---
- hosts: localhost
  gather_facts: false

  tasks:
    - name: Download apt key
      get_url:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        dest: /tmp # or /etc/pki/rpm-gpg depending on the infrastructure

    - name: Add a key from a file
      ansible.builtin.apt_key:
        file: /tmp/apt-key.gpg
        state: present

    - name: Update the apt package index
      tags: experiment
      become: yes
      apt:
        name: "*"
        state: latest
        update_cache: yes
        force_apt_get: yes

    - name: install required modules
      tags: experiment
      become: true
      apt:
        name: ['git', 'apt-transport-https', 'ca-certificates', 'wget', 'software-properties-common', 'gnupg2', 'curl', 'make', 'gcc', 'autoconf', 'automake', 'libtool', 'g++', 'protobuf-compiler', 'python3-pip']

    - name: Install Pillow for Python3
      tags: experiment
      pip:
        name: Pillow
        executable: pip3

    - name: Install TensorFlow for Python3
      tags: experiment
      pip:
        name: TensorFlow
        executable: pip3

    - name: Install pycocotools for Python3
      tags: experiment
      pip:
        name: pycocotools
        executable: pip3

    - name: Install pandas for Python3
      tags: experiment
      pip:
        name: pandas
        executable: pip3

    - name: enable sudoless invocation for perf
      tags: experiment
      become: true
      shell: |
        sh -c echo -1 >/proc/sys/kernel/perf_event_paranoid
        sysctl -w kernel.perf_event_paranoid=-1

    - name: add apt signing key from official docker repo
      tags: experiment
      become: true
      apt_key:
        url: https://download.docker.com/linux/debian/gpg
        state: present

    - name: add docker official repository for Debian Stretch
      tags: experiment
      become: true
      apt_repository:
        repo: deb [arch=amd64] https://download.docker.com/linux/debian stretch stable
        state: present

    - name: Index new repo into the cache
      tags: experiment
      become: yes
      apt:
        name: "*"
        state: latest
        update_cache: yes
        force_apt_get: yes

    - name: install docker
      tags: experiment
      become: true
      apt:
        name: "docker-ce"
        state: latest

    - name: ensure docker deamon is running
      tags: experiment
      become: yes
      service:
        name: docker
        state: started

    - name: add user to docker group
      tags: experiment
      become: yes
      shell: |
        groupadd docker
        usermod -aG docker {{ user }}
    - name: define variable for nvidia-docker
      tags: experiment
      stat:
        path: ./nvidia-docker
      register: nvidiadocker

    - name: clone and install nvidia-docker
      tags: experiment
      when: not nvidiadocker.stat.exists
      become: true
      become_user: "{{ user }}"
      shell: |
        distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
              && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
              && curl -s -L https://nvidia.github.io/libnvidia-container/experimental/$distribution/libnvidia-container.list | \
                 sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
                 sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
        cd nvidia-docker
        make

    - name: define variable for DeepLearningExamples
      tags: experiment
      stat:
        path: ./DeepLearningExamples
      register: deep_learning_examples

    - name: clone repo DeepLearningExamples
      tags: experiment
      when: not deep_learning_examples.stat.exists
      become: true
      become_user: "{{ user }}"
      shell: git clone https://github.com/NVIDIA/DeepLearningExamples.git


    # Build and configure Transformer-XL (Pytroch)
    - name: download data-set and build docker image for Pytorch's Transformer-XL
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/pytorch_transformer_to_wt103_base.yml DeepLearningExamples/PyTorch/LanguageModeling/Transformer-XL/pytorch/wt103_base.yaml
        cd DeepLearningExamples/PyTorch/LanguageModeling/Transformer-XL
        mkdir data || echo Already exists
        cd data
        wget --continue https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip
        unzip -q wikitext-103-v1.zip
        cd wikitext-103
        mv wiki.train.tokens train.txt
        mv wiki.valid.tokens valid.txt
        mv wiki.test.tokens test.txt
        cd ../..
        rm data/wikitext-103-v1.zip
      register: transformer_pytorch_output

    - debug:
        msg: "{{ transformer_pytorch_output.stdout_lines|list }}"
      tags: experiment

    # Build, configure, and generate train data-set corpuse.json for Transformer-XL (Tensorflow)
    - name: download data-set and build docker image for TensorFlow's Transformer-XL
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/tensorflow_transformer_to_run_wt103_base.sh DeepLearningExamples/TensorFlow/LanguageModeling/Transformer-XL/tf/run_wt103_base.sh
        cd DeepLearningExamples/TensorFlow/LanguageModeling/Transformer-XL
        mkdir data || echo Already exist
        cd data
        wget --continue https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip
        unzip -q wikitext-103-v1.zip
        cd wikitext-103
        mv wiki.train.tokens train.txt
        mv wiki.valid.tokens valid.txt
        mv wiki.test.tokens test.txt
        cd ../..
        rm data/wikitext-103-v1.zip
        bash tf/scripts/docker/build.sh
        docker run --tty --rm --gpus 1 --init --network=host  --ipc=host -v $PWD:/workspace/transformer-xl transformer-xl bash run_wt103_base.sh train_data --batch_chunk 32 --train_batch_size 16
      register: transformer_tensorflow_output

    - debug:
        msg: "{{ transformer_tensorflow_output.stdout_lines|list }}"
      tags: experiment

    # Build and configure SSD (PyTorch)
    - name: download dataset and build docker image for PyTorch's SSD
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/pytorch_ssd_SSD300_FP16_1GPU.sh DeepLearningExamples/PyTorch/Detection/SSD/examples/SSD300_FP16_1GPU.sh
        cd DeepLearningExamples/PyTorch/Detection/SSD
        mkdir coco2017 || echo Already exists
        bash download_dataset.sh ./coco2017
      register: ssd_pytorch_output

    - debug:
        msg: "{{ ssd_pytorch_output.stdout_lines|list }}"
      tags: experiment

    # Build and configure SSD (TensorFlow)
    - name: download dataset and build docker image for TensorFlows's SSD
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/tensorflow_ssd_ssd320_full_1gpus.config DeepLearningExamples/TensorFlow/Detection/SSD/configs/ssd320_full_1gpus.config
        cp configs/tensorflow_ssd_SSD320_FP16_1GPU.sh DeepLearningExamples/TensorFlow/Detection/SSD/examples/SSD320_FP16_1GPU.sh
        cd DeepLearningExamples/TensorFlow/Detection/SSD
        rm -rf coco2017 checkpoints || echo dirs not found
        mkdir coco2017 checkpoints
        sed -i 's/COPY\ qa\/\ qa\///g' Dockerfile
        docker build . -t nvidia_ssd
        bash download_all.sh nvidia_ssd $PWD/coco2017 $PWD/checkpoints
      register: ssd_tensorflow_output

    - debug:
        msg: "{{ ssd_tensorflow_output.stdout_lines|list }}"
      tags: experiment

    # Build and configure MaskRCNN (PyTorch)
    - name: download dataset and build docker image for PyTorch's MaskRCNN
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/pytorch_mask_r_cnn_e2e_mask_rcnn_R_50_FPN_1x_1GPU.yaml DeepLearningExamples/PyTorch/Segmentation/MaskRCNN/pytorch/configs
        cp configs/pytorch_mask_r_cnn_paths_catalog.py DeepLearningExamples/PyTorch/Segmentation/MaskRCNN/pytorch/maskrcnn_benchmark/config/paths_catalog.py
        cd DeepLearningExamples/PyTorch/Segmentation/MaskRCNN
        cp -r ../../Detection/SSD/coco2017 ./data
      register: maskrcnn_pytorch_output

    - debug:
        msg: "{{ maskrcnn_pytorch_output.stdout_lines|list }}"
      tags: experiment

    # Build and configure ResNet50 (PyTorch)
    - name: download dataset and build docker image for PyTorch's ResNet50
      tags: experiment
      become: true
      become_user: "{{ user }}"
      shell: |
        cp configs/pytorch_rn50_DGX1V_resnet50_AMP_90E.sh DeepLearningExamples/PyTorch/Classification/ConvNets/resnet50v1.5/training/AMP/DGX1V_resnet50_AMP_90E.sh
        cd DeepLearningExamples/PyTorch/Classification/ConvNets
        cp -r ../../Segmentation/MaskRCNN/download_dataset.sh ./
        bash download_dataset.sh coco2014
      register: rn_pytorch_output

    - debug:
        msg: "{{ rn_pytorch_output.stdout_lines|list }}"
      tags: experiment